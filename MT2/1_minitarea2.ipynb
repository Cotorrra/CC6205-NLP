{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "minitarea2.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2obO44rRIDIm",
    "colab_type": "text"
   },
   "source": [
    "# Minitarea 2\n",
    "\n",
    "Nombre: Joaquín Ignacio Pérez Araya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JstKx3TiKcIp",
    "colab_type": "text"
   },
   "source": [
    "---------------------------\n",
    "## Language Models (3 pts)\n",
    "Estas preguntas son teóricas y deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas. El material del curso y los videos deberian ser suficientes para que puedan responder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hwW-07MrRpt",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Asuma que tiene un modelo de lenguaje de trigramas (simple) entrenado sobre un corpus C. Muestre cómo el modelo le asigna una probabilidad a la oración `el perro le ladra al gato` usando los parámetros estimados a partir del corpus  $q(w_i | w_{i-2}, w_{i-1})$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VUjDx0NrYbg",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Se adiciona el token especial `STOP` a los tokens de la oración.\n",
    "Así lo que se calcula es: $P(el,perro,le,ladra,al,gato,STOP)$.\n",
    "\n",
    "Se le agregan los tokens extras de Padding, así:\n",
    "\n",
    "$P(*,*,el,perro,le,ladra,al,gato,STOP)$\n",
    "\n",
    "Se utiliza regla de la cadena y usando un modelo de Markov de Segundo Orden:\n",
    "\n",
    "$P(el|*,*) \\times P(perro|*,el) \\times P(le|el,perro) \\times P(ladra|perro,le) \\times $\n",
    "$P(al| le,ladra) \\times P(gato|ladra,al) \\times P(STOP| al,gato )$\n",
    "\n",
    "Estas son equivalentes a los parámetros obtenidos del corpus $C$:\n",
    "$q(el|*,*) \\times q(perro|*,el) \\times q(le|el,perro) \\times q(ladra|perro,le) \\times $\n",
    "$q(al| le,ladra) \\times q(gato|ladra,al) \\times q(STOP| al,gato )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwNkPIXure0C",
    "colab_type": "text"
   },
   "source": [
    "### Pregunta 2 (1 pt)\n",
    "Muestre cómo se calcularía  $q(w_{i} | w_{i-2}, w_{i-1})$  usando un modelo que interpola un modelo de lenguajes de trigramas, bigramas, y unigrama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeLZAd0Tr9ne",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Supongamos que se tienen 3 modelos de lenguajes: Uno con Trigramas ($T$), otro con Bigramas ($B$) y uno con Unigramas ($U$), \n",
    "con sus respectivas probabilidades $q_T(w_i|w_{i-2},w_{i-1})$, $q_B(w_i|w_{i-1})$ y $q_U(w_i|w_{i-1})$ respectivamente, que pueden ser calculados de diferentes formas.\n",
    "\n",
    "Sea $\\lambda_1,\\lambda_2,\\lambda_3\\ge 0$ tal que $\\lambda_1+\\lambda_2+\\lambda_3 = 1$\n",
    "\n",
    "$q(w_{i} | w_{i-2}, w_{i-1})$ será calculado a partir de un interpolación lineal de las probabilidades de mencionadas anteriomente, es decir: \n",
    "\n",
    "$q(w_{i} | w_{i-2}, w_{i-1}) = \\lambda_1 \\cdot q_U(w_{i}) + \\lambda_2 \\cdot q_B(w_{i}|w_{i-1}) + \\lambda_3 \\cdot q_B(w_{i}|w_{i-2},w_{i-1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHqcRJ7Vr_8x",
    "colab_type": "text"
   },
   "source": [
    "### Pregunta 3 (1 pt)\n",
    "¿Qué ventajas tiene el modelo interpolado sobre el modelo de trigramas simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F5R3Ji7sHjt",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta:**\n",
    "El modelo interpolado es más flexible a los casos cuando se le calculan probablidades de oraciones (en particular trigramas) que no existen en el Corpus de entrenamiento. \n",
    "De esta forma no se da probabilidad $0$ a oraciones que tengan \n",
    "secuencias de palabras que no se hayan visto nunca en el entrenamiento, que es factible que ocurra en realidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdmB-07ZKfaa",
    "colab_type": "text"
   },
   "source": [
    "-----------------------\n",
    "## Naive Bayes (3 pts)\n",
    "En esta parte de la minitarea deberan programar Naive Bayes Multinomial usando Laplace Smothing. Las referencias las pueden encontrar en el material del curso y los videos del profesor.\n",
    "\n",
    "A continuacion se presentan un conjunto de documentos de training divididos en 2 categorias distintas. Ustedes deberan clasificar los documentos del test set usando Naive Bayes con Laplace Smothing.\n",
    "\n",
    "Por favor, documenten su codigo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algun lugar de su codigo hacen algo que creen que debe ser explicado, pongan comentarios, son bienvenidos. \n",
    "\n",
    "\n",
    "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplacenlas por sumas de logaritmos para prevenir errores de precision. Revisen la diapo 69 de las slides. \n",
    "\n",
    "NOTA: Sobre las `namedtuple`s. Es el tipo de los documentos. Son objetos inmutable que tienen dos atributos: `words` donde estan las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`. Son libres de implementar su solucion como quieran, si no les gusta este tipo de dato usen otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "outputId": "efd50e50-3317-454b-c74f-222458249c48",
    "id": "HLi8PxdV2VQX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = (\n",
    "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
    "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
    "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
    "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
    "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
    "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
    "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
    "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
    ")\n",
    "\n",
    "print(\"Train documents:\")\n",
    "pprint(train_set)\n",
    "\n",
    "\n",
    "\n",
    "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n",
    "print(\"\\nTest documents:\")\n",
    "pprint(test_set)\n"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train documents:\n(document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1))\n\nTest documents:\n(document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "La implementación se realizó creando una clase que implementa Naive Bayes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0edu0E7LA3U9",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Naive_Bayes:\n",
    "    count_data = pd.DataFrame() # Conteo de palabras bajo una clase\n",
    "    classes = [] # Rotulos de clases\n",
    "    vocab = []   # Palabras del Vocabulario\n",
    "    class_count = [] # Conteo total de clases\n",
    "    word_count = []  # Conteo total de palabras\n",
    "    \n",
    "    def __init__(self, train):\n",
    "        # Get classes + Get Vocab\n",
    "        for T in train:\n",
    "            if not T.class_ in self.classes:\n",
    "                self.classes += [T.class_]\n",
    "                \n",
    "            for w in T.words:\n",
    "                if not w in self.vocab:\n",
    "                    self.vocab += [w]\n",
    "            \n",
    "        # Create dataframe\n",
    "        self.count_data = pd.DataFrame(columns=self.vocab, index=self.classes)\n",
    "        \n",
    "        # Count total words ocurrences and class ocurrences\n",
    "        self.word_count = np.zeros(len(self.vocab))\n",
    "        self.class_count = np.zeros(len(self.classes))         \n",
    "        for T in train:\n",
    "            class_index = self.classes.index(T.class_)\n",
    "            self.class_count[class_index] += 1\n",
    "            for w in T.words:\n",
    "                word_index = self.vocab.index(w)\n",
    "                self.word_count[word_index] += 1\n",
    "        \n",
    "        # Count words with their respective class\n",
    "        for c in self.classes:\n",
    "            count = np.zeros(len(self.vocab))\n",
    "            for T in train:\n",
    "                if T.class_ == c:\n",
    "                    for w in T.words:\n",
    "                        count[self.vocab.index(w)] += 1\n",
    "            self.count_data.loc[c] = count\n",
    "\n",
    "\n",
    "    def class_prob(self, class_): # P(c)\n",
    "        class_index = self.classes.index(class_)\n",
    "        return self.class_count[class_index] / sum(self.class_count)\n",
    "            \n",
    "\n",
    "    def word_prob(self, word, c): # P(w|c)\n",
    "        word_index = self.vocab.index(word)\n",
    "        word_class_count = self.count_data.loc[c][word_index]\n",
    "        word_total_count = self.word_count[word_index]\n",
    "        # Retorno la probablidad con LaPlace Smoothing\n",
    "        return (word_class_count + 1) / (word_total_count + len(self.vocab))\n",
    "\n",
    "    def get_class(self, words): \n",
    "        classValue = np.zeros(len(self.classes))\n",
    "        # Por cada clase calcula el P(c|d=w1, w2,...,wn)\n",
    "        for c in self.classes:\n",
    "            class_index = self.classes.index(c)\n",
    "            # Calculo su valor que es la suma de los logs\n",
    "            classValue[class_index] += np.log2(self.class_prob(c))\n",
    "            for w in words:\n",
    "                classValue[class_index] += np.log2(self.word_prob(w,c))\n",
    "        \n",
    "        # Retorno la que tiene mayor score\n",
    "        return self.classes[np.argmax(classValue)]\n",
    "                        \n",
    "myBayes = Naive_Bayes(train_set)\n",
    "myBayes.get_class(test_set[0].words)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "-20.49685377738804"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ]
  }
 ]
}