{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objetivo-e-Instrucciones:\" data-toc-modified-id=\"Objetivo-e-Instrucciones:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objetivo e Instrucciones:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Fecha-de-Entrega:\" data-toc-modified-id=\"Fecha-de-Entrega:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Fecha de Entrega:</a></span></li><li><span><a href=\"#Detalles-e-instrucciones-de-la-competencia:\" data-toc-modified-id=\"Detalles-e-instrucciones-de-la-competencia:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Detalles e instrucciones de la competencia:</a></span></li><li><span><a href=\"#Reporte\" data-toc-modified-id=\"Reporte-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Reporte</a></span></li><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Baseline</a></span></li></ul></li><li><span><a href=\"#1.-Introducci贸n\" data-toc-modified-id=\"1.-Introducci贸n-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1. Introducci贸n</a></span></li><li><span><a href=\"#2.-Representaciones\" data-toc-modified-id=\"2.-Representaciones-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2. Representaciones</a></span></li><li><span><a href=\"#3.-Algoritmos\" data-toc-modified-id=\"3.-Algoritmos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3. Algoritmos</a></span></li><li><span><a href=\"#4.-M茅tricas-de-Evaluaci贸n\" data-toc-modified-id=\"4.-M茅tricas-de-Evaluaci贸n-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4. M茅tricas de Evaluaci贸n</a></span></li><li><span><a href=\"#5.-Experimentos\" data-toc-modified-id=\"5.-Experimentos-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>5. Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librer铆as-y-utiles\" data-toc-modified-id=\"Importar-librer铆as-y-utiles-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Importar librer铆as y utiles</a></span></li><li><span><a href=\"#Definir-m茅todos-de-evaluaci贸n\" data-toc-modified-id=\"Definir-m茅todos-de-evaluaci贸n-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Definir m茅todos de evaluaci贸n</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representaci贸n-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representaci贸n-y-el-clasificador-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Definir la representaci贸n y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-alg煤n-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-alg煤n-dataset-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Ejecutar el pipeline para alg煤n dataset</a></span></li><li><span><a href=\"#Ejecutar-el-sistema-creado-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-sistema-creado-por-cada-train-set-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Ejecutar el sistema creado por cada train set</a></span></li><li><span><a href=\"#Predecir-los-target-set-y-crear-la-submission\" data-toc-modified-id=\"Predecir-los-target-set-y-crear-la-submission-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Predecir los target set y crear la submission</a></span></li></ul></li><li><span><a href=\"#6.-Conclusiones\" data-toc-modified-id=\"6.-Conclusiones-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>6. Conclusiones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    }
   },
   "source": [
    "# Tarea 1 NLP : Competencia de Clasificaci贸n de Texto\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nombre:**\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo e Instrucciones:\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificaci贸n de tweets seg煤n su intensidad de emoci贸n. Espec铆ficamente: \n",
    "\n",
    "Tendr谩n 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deber谩n crear un clasificador que indique la intensidad de dicha emoci贸n en sus tweets (`low`, `medium`, `high`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fecha de Entrega: \n",
    "\n",
    "Por ser anunciada una vez termine el paro. Se publicar谩 la fecha en ucursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:34:38.796217Z",
     "start_time": "2020-04-07T14:34:38.782255Z"
    }
   },
   "source": [
    "### Detalles e instrucciones de la competencia:\n",
    "\n",
    "- La competencia consiste en resolver 4 problemas de clasificaci贸n distintos, cada uno de tres clases. Por cada problema deber谩n crear un clasificador distinto. La evaluaci贸n de la competencia se realiza en base a 4 m茅tricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada 铆tem ser谩n los que ganen.\n",
    "\n",
    "- Para comenzar se les entregar谩 en este notebook el baseline y la estructura del reporte. El baseline es el c贸digo que realiza creaci贸n de features y clasificaci贸n b谩sica. Los puntajes de este ser谩n ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
    "\n",
    "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
    "\n",
    "- **Es requisito entregar el reporte con el c贸digo y haber participado en la competencia para ser evaluado.**\n",
    "\n",
    "- Pueden hacer grupos de m谩ximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y despu茅s cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
    "\n",
    "- En total pueden hacer un **m谩ximo de 4 env铆os/submissions** (tanto para equipos como para env铆os indivuales).\n",
    "\n",
    "- Hagan varios experimentos haciendo cross-validation o evaluaci贸n sobre una sub-partici贸n antes de enviar sus predicciones a Codalab. Aseg煤rense que la distribuci贸n de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les ser谩 evaluado incorrectamente.\n",
    "\n",
    "- Estar top 5 en alguna m茅trica equivale a 1 punto extra en la nota final.\n",
    "\n",
    "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. 隆Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
    "\n",
    "- Todas las dudas escr铆banlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente ser谩n remitidos a ese medio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporte\n",
    "\n",
    "Este debe cumplir la siguiente estructura:\n",
    "\n",
    "1.\t**Introducci贸n**: Presentar brevemente el problema a resolver, los m茅todos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci贸n agregando m谩s atributos y representaciones dise帽adas a mano. Mas abajo encontrar谩n una lista 煤til de estos que les podr谩 ser de utilidad. (1.5 puntos)\n",
    "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificaci贸n usados. (0.5 puntos)\n",
    "4.\t**M茅tricas de evaluaci贸n**: Describir brevemente las m茅tricas utilizadas en la evaluaci贸n indicando que miden y su interpretaci贸n. (0.5 puntos)\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partici贸n de evaluaci贸n que deben crear (o pueden usar cross-validation). Incluyan todo el c贸digo de sus experimentos aqu铆. 隆Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    }
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Por 煤ltimo, el baseline contiene un c贸digo b谩sico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words (BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se est谩 ejecutando el notebook. \n",
    "\n",
    "\n",
    "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendr谩 mas sentido cuando vean el c贸digo)\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi茅n, el par谩metro `ngram_range` (Ojo que el clf naive bayes no deber铆a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem谩s, implementar los atributos que crean 煤tiles desde el listado del el enunciado. Investigar tambi茅n el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci贸n `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aqu铆 les adjuntamos algunos ejemplos:\n",
    "    -\tWord n-grams.\n",
    "    -\tCharacter n-grams. \n",
    "    -\tPart-of-speech tags.\n",
    "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
    "        - Count the number of positive and negative words within a sentence.\n",
    "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
    "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
    "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
    "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
    "    -\tThe number of words with all characters in uppercase.\n",
    "    -\tThe presence and the number of positive or negative emoticons.\n",
    "    -\tThe number of individual negations.\n",
    "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
    "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
    "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
    "\n",
    "- **Reducci贸n de dimensionalidad**: Tambi茅n puede serles de ayuda. Referencias [aqu铆](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por 煤ltimo, pueden encontrar mas referencias de c贸mo mejorar sus features, el vectorizador y el clasificador [aqu铆](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:25:19.677190Z",
     "start_time": "2020-04-07T15:25:19.671206Z"
    }
   },
   "source": [
    "(Pueden eliminar cualquier celda con instrucciones...)\n",
    "\n",
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO ser谩n evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    }
   },
   "source": [
    "## 1. Introducci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    }
   },
   "source": [
    "## 2. Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    }
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    }
   },
   "source": [
    "## 4. M茅tricas de Evaluaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    }
   },
   "source": [
    "### Importar librer铆as y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.587160Z",
     "start_time": "2020-04-07T15:44:19.319386Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir m茅todos de evaluaci贸n\n",
    "\n",
    "Estas funciones est谩n a cargo de evaluar los resultados de la tarea. No deber铆an cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:20.604066Z",
     "start_time": "2020-04-07T15:44:20.589106Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.068137Z",
     "start_time": "2020-04-07T15:44:20.606061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets de entrenamiento.\n",
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "# Datasets que deber谩n predecir para la competencia.\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.088707Z",
     "start_time": "2020-04-07T15:44:21.069757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                              tweet  class  \\\n226  10226  Just paid for chicken at @panerabread and didn...  anger   \n602  10602  remember when that guy got #angry that i kept ...  anger   \n4    10004  Don't join @BTCare they put the phone down on ...  anger   \n837  10837  Follow me on Snapchat  Carlabtst15 #snapchat ...  anger   \n135  10135  @dcexaminer Democrats and their voters have ze...  anger   \n\n    sentiment_intensity  \n226              medium  \n602              medium  \n4                  high  \n837                 low  \n135                high  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>sentiment_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>226</th>\n      <td>10226</td>\n      <td>Just paid for chicken at @panerabread and didn...</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>602</th>\n      <td>10602</td>\n      <td>remember when that guy got #angry that i kept ...</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10004</td>\n      <td>Don't join @BTCare they put the phone down on ...</td>\n      <td>anger</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>837</th>\n      <td>10837</td>\n      <td>Follow me on Snapchat  Carlabtst15 #snapchat ...</td>\n      <td>anger</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>10135</td>\n      <td>@dcexaminer Democrats and their voters have ze...</td>\n      <td>anger</td>\n      <td>high</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "train['anger'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, seg煤n su intensidad de sentimiento. Noten que las clases est谩n desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.117633Z",
     "start_time": "2020-04-07T15:44:21.090703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta car谩cteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una funci贸n c贸mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la funci贸n `transform` en donde iteramos por cada tweet, llamamos a la funci贸n que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitar谩 el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu铆](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.128600Z",
     "start_time": "2020-04-07T15:44:21.119624Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        return [num_hashtags, num_exclamations, num_interrogations]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.145564Z",
     "start_time": "2020-04-07T15:44:21.131593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   0          1\n0  @bt_uk why does tracking show my equipment del...  [1, 0, 2]\n1  Projection is perception. See it in someone el...  [1, 0, 1]\n2  Houston might lose a coach tomorrow or by midn...  [2, 0, 1]\n3  @CozanGaming that's what lisa asked before she...  [0, 0, 1]\n4  I wonder what would happen if I were to tell s...  [3, 0, 0]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@bt_uk why does tracking show my equipment del...</td>\n      <td>[1, 0, 2]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Projection is perception. See it in someone el...</td>\n      <td>[1, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Houston might lose a coach tomorrow or by midn...</td>\n      <td>[2, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@CozanGaming that's what lisa asked before she...</td>\n      <td>[0, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I wonder what would happen if I were to tell s...</td>\n      <td>[3, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "sample = train['anger'].sample(5).tweet\n",
    "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la representaci贸n y el clasificador\n",
    "\n",
    "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as铆 nuestra programaci贸n.\n",
    "\n",
    "El pipeline m谩s b谩sico que podemos hacer es transformar el dataset a Bag of Words y despu茅s usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformaci贸n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar谩 los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
    "\n",
    "```python\n",
    "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                        ('chars_count',CharsCountTransformer())])),\n",
    "              ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden que cada pipeline representa un sistema de clasificaci贸n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr铆an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.155528Z",
     "start_time": "2020-04-07T15:44:21.149545Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_experiment_0_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf',  LogisticRegression(max_iter=1000000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el pipeline para alg煤n dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.167498Z",
     "start_time": "2020-04-07T15:44:21.157540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name, pipeline):\n",
    "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
    "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
    "\n",
    "    # Dividimos el dataset en train y test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "\n",
    "    # Evaluamos:\n",
    "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el sistema creado por cada train set\n",
    "\n",
    "Este c贸digo crea y entrena los 4 sistemas de clasificaci贸n y luego los evalua. Para los experimentos, pueden copiar este c贸digo variando el pipeline cuantas veces estimen conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.384119Z",
     "start_time": "2020-04-07T15:44:21.170488Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  8  46   0]\n",
      " [ 14 172  14]\n",
      " [  1  37  19]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.35      0.15      0.21        54\n",
      "      medium       0.67      0.86      0.76       200\n",
      "        high       0.58      0.33      0.42        57\n",
      "\n",
      "    accuracy                           0.64       311\n",
      "   macro avg       0.53      0.45      0.46       311\n",
      "weighted avg       0.60      0.64      0.60       311\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.639\tKappa: 0.182\tAccuracy: 0.64\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 33  78   2]\n",
      " [ 23 171  26]\n",
      " [  7  41  34]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.52      0.29      0.38       113\n",
      "      medium       0.59      0.78      0.67       220\n",
      "        high       0.55      0.41      0.47        82\n",
      "\n",
      "    accuracy                           0.57       415\n",
      "   macro avg       0.55      0.49      0.51       415\n",
      "weighted avg       0.56      0.57      0.55       415\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.691\tKappa: 0.237\tAccuracy: 0.573\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 24  47   2]\n",
      " [ 30 121  23]\n",
      " [  2  28  21]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.43      0.33      0.37        73\n",
      "      medium       0.62      0.70      0.65       174\n",
      "        high       0.46      0.41      0.43        51\n",
      "\n",
      "    accuracy                           0.56       298\n",
      "   macro avg       0.50      0.48      0.49       298\n",
      "weighted avg       0.54      0.56      0.55       298\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.684\tKappa: 0.185\tAccuracy: 0.557\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 18  45   1]\n",
      " [ 17 121  10]\n",
      " [  3  42  27]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.47      0.28      0.35        64\n",
      "      medium       0.58      0.82      0.68       148\n",
      "        high       0.71      0.38      0.49        72\n",
      "\n",
      "    accuracy                           0.58       284\n",
      "   macro avg       0.59      0.49      0.51       284\n",
      "weighted avg       0.59      0.58      0.56       284\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.715\tKappa: 0.25\tAccuracy: 0.585\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.682\t Average Kappa: 0.213\t Average Accuracy: 0.589\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "scores_array = []\n",
    "\n",
    "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for dataset_name, dataset in train.items():\n",
    "    \n",
    "    # creamos el pipeline\n",
    "    pipeline = get_experiment_0_pipeline()\n",
    "    \n",
    "    # ejecutamos el pipeline sobre el dataset\n",
    "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n",
    "\n",
    "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # guardamos las labels aprendidas por el clasificador\n",
    "    learned_labels_array.append(learned_labels)\n",
    "\n",
    "    # guardamos los scores obtenidos\n",
    "    scores_array.append(scores)\n",
    "\n",
    "# print avg scores\n",
    "print(\n",
    "    \"Average scores:\\n\\n\",\n",
    "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "    .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    }
   },
   "source": [
    "### Predecir los target set y crear la submission\n",
    "\n",
    "Aqu铆 predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.392097Z",
     "start_time": "2020-04-07T15:44:21.386114Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:44:21.588573Z",
     "start_time": "2020-04-07T15:44:21.394094Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.exists('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivos separados. \n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-959495b3",
   "language": "python",
   "display_name": "PyCharm (CC6205-NLP)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}