{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Auxiliar 5.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "pycharm-959495b3",
   "language": "python",
   "display_name": "PyCharm (CC6205-NLP)"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "6912a31b002244ba9878dd0f5e879e65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_e3a3aa3148d14161a9bfaab79f6c2498",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_5a92f2c65b5d44bdb55db4c70a10acbd",
       "IPY_MODEL_8d9fa6bb830b4c968ed07cad332a253f"
      ]
     }
    },
    "e3a3aa3148d14161a9bfaab79f6c2498": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "5a92f2c65b5d44bdb55db4c70a10acbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_76c538b27def4a6d8f33d28924d27210",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 231508,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 231508,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_15c358c34e7d4d179ad69203683b1f57"
     }
    },
    "8d9fa6bb830b4c968ed07cad332a253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_9cec363e9c1945aca4cd65895fff9d62",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 232k/232k [00:00&lt;00:00, 373kB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_145b9b82731d4611a27a23614181fbf2"
     }
    },
    "76c538b27def4a6d8f33d28924d27210": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "15c358c34e7d4d179ad69203683b1f57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "9cec363e9c1945aca4cd65895fff9d62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "145b9b82731d4611a27a23614181fbf2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "d8f1ea3f6f2547c8b4bf6db95c783022": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_4ccdda4320954b878078608871d4533b",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_afb2f2d2e3f741c69c24c9e60cb4a3db",
       "IPY_MODEL_68b29443ce81485f99e3e889555d432c"
      ]
     }
    },
    "4ccdda4320954b878078608871d4533b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "afb2f2d2e3f741c69c24c9e60cb4a3db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_8e6613fe1852449c95fbb92810bfdcd0",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 433,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 433,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_83570edef12947c9a3f27b6d7c3d38fe"
     }
    },
    "68b29443ce81485f99e3e889555d432c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_225e29d32b70439ebf461b4219d3c12e",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 433/433 [00:10&lt;00:00, 41.8B/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_4f28403a735a48c3b91ede9cfd9d417d"
     }
    },
    "8e6613fe1852449c95fbb92810bfdcd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "83570edef12947c9a3f27b6d7c3d38fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "225e29d32b70439ebf461b4219d3c12e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "4f28403a735a48c3b91ede9cfd9d417d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "af2f62aec57d4beea2f6294cc0dda376": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ba4d288e623f4ca58f094100788abcd4",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_88613a49b3e3438cb452eead274a52a1",
       "IPY_MODEL_dc8fbc48b3ee40f08f1ea6adb7d73ef7"
      ]
     }
    },
    "ba4d288e623f4ca58f094100788abcd4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "88613a49b3e3438cb452eead274a52a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_1f3d10e74b0c4dd0a7cb060e3e9d43fb",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 440473133,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 440473133,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_b26447849a5d48b6bc3171b70c44f9b2"
     }
    },
    "dc8fbc48b3ee40f08f1ea6adb7d73ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_5f545f04bcfd4843a103f92b85188151",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 440M/440M [00:09&lt;00:00, 44.2MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_d283019c74bc405aadd9bfddc72e4008"
     }
    },
    "1f3d10e74b0c4dd0a7cb060e3e9d43fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "b26447849a5d48b6bc3171b70c44f9b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "5f545f04bcfd4843a103f92b85188151": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "d283019c74bc405aadd9bfddc72e4008": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLLTxJRSwscP",
    "colab_type": "text"
   },
   "source": [
    "#Introducción\n",
    "\n",
    "<!-- Aquí la idea es mostrarles como pueden usar BERT y quizás BETO en\n",
    "español usando la librería Transformers. Me interesa que lo usen\n",
    "de dos formas:\n",
    "\n",
    "1) como extractor de vectores contextualizados\n",
    "\n",
    "2) para hacer fine-tuning a otra task (e.g., Question Answering).\n",
    " -->\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "En esta auxiliar vamos a utilizar BERT, un modelo de lenguaje desarrollado por Google. Este modelo rompió varios récords en NLP y de hecho, cada vez que buscan en Google, BERT ayuda a refinar sus búsquedas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2EvTlRNbfSV",
    "colab_type": "text"
   },
   "source": [
    "## Pero, qué es BERT?\n",
    "\n",
    "BERT es un personaje de plaza sésamo, al igual que ELMo, los cuales saben mucho de lenguaje y podemos ver con una mirada desafiante en la siguiente imagen:\n",
    "\n",
    "![bert y elmo](https://i.imgur.com/1T4kyrq.png)\n",
    "\n",
    "Los genios dándole nombres a los papers decidieron que era buena idea que los acrónimos se refirieran a los personajes de plaza sesamo, con los cuales algunos de nosotros (los más viejos) aprendimos a hablar y deletrear ~~hasta quizás mejor que en el jardín infantil~~. Al igual que Elmo, Embeddings from Language MOdels, BERT es el acrónimo de Bidirectional Encoder Representations from Transformers. \\\n",
    "Estos dos modelos producen \"contextualized word embeddings\". A diferencia de los modelos que producen static word embeddings como Word2Vec, la representación no depende solo de la palabra, sino que de la palabra y su contexto. Por lo tanto, cada palabra tiene infinitas representaciones, lo cual es mucho más flexible que tener solo un vector para cada palabra.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woZQw3j2bkPw",
    "colab_type": "text"
   },
   "source": [
    "## Qué significa Bidirectional Encoder Representations from Transformers?\n",
    "A diferencia de ELMo, el cual era una concatenación de información de izquierda-derecha y derecha-izquierda, BERT es bidireccional, es decir, toma en cuenta los contextos a la izquierda y derecha de la palabra simultáneamente.\n",
    "\n",
    "BERT además utiliza Transformers, arquitecturas de deep learning altamente paralelizables que cuentan con un proceso de Encoder-Decoder. Dado que el objetivo de BERT es generar un modelo de lenguaje, solo es necesario el mecanismo de Encoding y le dejan el proceso de Decoding a las distintas tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-CrbVL5bsrm",
    "colab_type": "text"
   },
   "source": [
    "## Y como fue entrenado?\n",
    "\n",
    "El primer objetivo de BERT es algo que se llama \"masked language modeling\". En este modelo, las palabras de una frase se borran al azar y se reemplazan por un token especial ([MASK]) con probabilidad 15%. Luego, se utiliza un Transformer para generar una predicción para la palabra remplazada por [MASK] basada en las palabras no enmascaradas que la rodean, tanto a la izquierda como a la derecha.\n",
    "\n",
    "El segundo objetivo de BERT es resolver la tarea de Next Sentence Prediction. El modelo recibe dos oraciones como entrada y aprende a predecir si la segunda oración del par es la oración que siguiente del documento original. Durante el entrenamiento, el 50% de los inputs son un par en el que la segunda frase es la frase siguiente en el documento original, mientras que en el otro 50% se elige una frase aleatoria del corpus como segunda frase.\n",
    "\n",
    "\n",
    "Pueden leer un poco más [acá](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqCm5gsibx5s",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "## Oye, pero esto suena un poco magico, tienes algunos ejemplos?\n",
    "\n",
    "Hay bastantes librerías que tienen el modelo pre-entrenado a disposición, partiendo por el [GitHub de BERT](https://github.com/google-research/bert) implementado en TensorFlow. Como nosotros sabemos utilizar pytorch, utilizaremos la [version de HuggingFace](https://huggingface.co/transformers/) la cual es respaldada por el github de Google y la elogian: \"which is compatible with our pre-trained checkpoints and is able to reproduce our results\". Esta version se importa con la libreria transformers. Otras version disponibles son [sentence-bert](https://github.com/UKPLab/sentence-transformers) o [bert-as-service](https://github.com/hanxiao/bert-as-service)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GXbAJvFZdRK6",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "outputId": "cbb46b63-54a4-4968-ddeb-4c501a0e1221"
   },
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: numpy in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp36-cp36m-win_amd64.whl (1.9 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: requests in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: six in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=a73a540cf50b8664da1b02cfd184aa88f33c9fd90fa8d110bb844ecda3c6d995\n",
      "  Stored in directory: c:\\users\\amitsuu\\appdata\\local\\pip\\cache\\wheels\\49\\25\\98\\cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, click, sacremoses, filelock, tokenizers, dataclasses, pyparsing, packaging, transformers\n",
      "Successfully installed click-7.1.2 dataclasses-0.7 filelock-3.0.12 packaging-20.4 pyparsing-2.4.7 regex-2020.7.14 sacremoses-0.0.43 tokenizers-0.8.1rc1 transformers-3.0.2\n",
      "Requirement already satisfied: torch in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: future in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in d:\\workspace\\python\\envs\\nlp\\lib\\site-packages (from torch) (1.18.5)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gsaS37KAzZoj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcHpz3iEc6-_",
    "colab_type": "text"
   },
   "source": [
    "Veamos el primer ejemplo que entrega la documentación"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HePHI6odsBc",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "6912a31b002244ba9878dd0f5e879e65",
      "e3a3aa3148d14161a9bfaab79f6c2498",
      "5a92f2c65b5d44bdb55db4c70a10acbd",
      "8d9fa6bb830b4c968ed07cad332a253f",
      "76c538b27def4a6d8f33d28924d27210",
      "15c358c34e7d4d179ad69203683b1f57",
      "9cec363e9c1945aca4cd65895fff9d62",
      "145b9b82731d4611a27a23614181fbf2",
      "d8f1ea3f6f2547c8b4bf6db95c783022",
      "4ccdda4320954b878078608871d4533b",
      "afb2f2d2e3f741c69c24c9e60cb4a3db",
      "68b29443ce81485f99e3e889555d432c",
      "8e6613fe1852449c95fbb92810bfdcd0",
      "83570edef12947c9a3f27b6d7c3d38fe",
      "225e29d32b70439ebf461b4219d3c12e",
      "4f28403a735a48c3b91ede9cfd9d417d",
      "af2f62aec57d4beea2f6294cc0dda376",
      "ba4d288e623f4ca58f094100788abcd4",
      "88613a49b3e3438cb452eead274a52a1",
      "dc8fbc48b3ee40f08f1ea6adb7d73ef7",
      "1f3d10e74b0c4dd0a7cb060e3e9d43fb",
      "b26447849a5d48b6bc3171b70c44f9b2",
      "5f545f04bcfd4843a103f92b85188151",
      "d283019c74bc405aadd9bfddc72e4008"
     ]
    },
    "outputId": "77141544-720a-4a78-f854-373740814336"
   },
   "source": [
    "# Si estamos utilizando google colab, no se preocupen por las descargas, ya que las hace el servidor de colab y no les gasta ancho de banda a uds\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Cargamos el tokenizador\n",
    "model = BertModel.from_pretrained('bert-base-uncased') # Cargamos el modelo pre-entrenado"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66fc5f04c0154e6297b57b614e9c118c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "030e1770881141a58ba19f7d4fe7cd8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7b40f18a12946eb9bb22bfe2b3ffd9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GwxJlH5Vc914",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "outputId": "e33bb411-399c-4de1-8494-b7e3ec29ab1c"
   },
   "source": [
    "# 'pt' especifica que queremos vectores de pytorch, 'tf' seria en tensorflow\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\") \n",
    "# el doble asterico ayuda a evaluar los valores de un diccionario, por ejemplo:\n",
    "# d = {'a': 1, 'b':2}, model(**d) sería equivalente a model(1,2)\n",
    "outputs = model(**inputs)\n",
    "# El ultimo hidden-state es el primer elemento del output del modelo.\n",
    "last_hidden_states = outputs[0].squeeze(0) # squeeze en la primera dimension ya que es 1\n",
    "print(inputs['input_ids']) # Tenemos 8 tokens, contando el CLS (101) y el SEP (102)\n",
    "print(last_hidden_states.shape) # Tenemos 8 vectores de 768 dimensiones\n",
    "print(last_hidden_states)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])\n",
      "torch.Size([8, 768])\n",
      "tensor([[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
      "        [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
      "        [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
      "        ...,\n",
      "        [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
      "        [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
      "        [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU4iTHzpbBjm",
    "colab_type": "text"
   },
   "source": [
    "La primera pregunta es... cómo diantres obtengo una representación de mi oración desde el último hidden-state?\n",
    "\n",
    "Las opciones más simples son tomar el token CLS, aunque no es muy recomendado, ya que depende del fine tunning (veremos esto más adelante) y la otra opción es tomar el promedio de todos mis tokens. Hay más formas de pooling (es decir como se mezclan los tokens), por ejemplo, podemos ver las de bert-as-service [acá](https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tbmxaTNpeeCI",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "0d22a0b5-ad8c-4ba3-e295-5c407e33cc2e"
   },
   "source": [
    "cantidad_tokens = inputs['input_ids'].shape[1]\n",
    "# Representacion con token cls\n",
    "cls_representation = last_hidden_states[0] # El primer token del ultimo hidden-state es el CLS\n",
    "print(cls_representation.shape) # Representacion de 768 dimensiones\n",
    "\n",
    "# Representacion con average 1 (más verbosa)\n",
    "average = torch.zeros(768)\n",
    "for i in range(1, cantidad_tokens-1): # Partimos en 1 y terminamos en largo-2 para ignorar CLS y SEP\n",
    "  average += last_hidden_states[i] \n",
    "average = average/(cantidad_tokens-2) # Obtenemos nuestra representacion de 768 dimensiones\n",
    "print(average.shape)\n",
    "\n",
    "# Representacion con average 2 (más corta)\n",
    "average2 = torch.mean(last_hidden_states[1:-1], 0)\n",
    "print(average2.shape)\n",
    "print(torch.equal(average, average2))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Noj-NgUarIXt",
    "colab_type": "text"
   },
   "source": [
    "## Bacán, puedo representar oraciónes con BERT, puedo hacer algo más a parte de eso o se acabó la diversión?\n",
    "\n",
    "¡Esto es solo el principio! ¡La librería transformers a parte nos presta modelos con decoders fine-tuneados en ciertas tareas y hasta podemos fine-tunearlos nosotros!\n",
    "\n",
    "Fine-tunear ayuda a que BERT entienda cual es la tarea que queremos resolver. El modelo de por sí ya viene pre-entrenado en las 2 tareas que mencione previamente, Masked Language Modeling y Next Sentence Prediction sobre corpus muy grandes. \n",
    "\n",
    "Hay 2 modelos pre-entrenados de BERT: bert-base y bert-large que difieren en el tamaño del modelo, pero fueron entrenados sobre el mismo corpus: Wikipedia en ingles, además de aproximadamente 11.000 libros en ingles (esto se llama BookCorpus).\n",
    "\n",
    "bert-base tiene 12 layers (transformer blocks), 12 attention heads, y 110 millones de parametros\n",
    "\n",
    "bert-large tiene 24 layers (transformer blocks), 16 attention heads, y 340 millones de parametros\n",
    "\n",
    "Una vez hice el cálculo rápido de cuanto me saldría entrenar bert-base desde cero en las cloud TPU de Google y era una cifra cercana a los 2000 dólares. Por suerte aquí tenemos a alguien que lo entrenó en español y nos puede contar su experiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3in0Nv1fZ9N",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "## Gabriel y BETO\n",
    "\n",
    "Experiencia de Gabriel y BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbgGNC8bsOp3",
    "colab_type": "text"
   },
   "source": [
    "Continuando con los ejemplos, veamos como sería utilizar BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i5Hgj5mtszxF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from transformers import BertForNextSentencePrediction"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiREjwJXs_J_",
    "colab_type": "text"
   },
   "source": [
    "Utilizamos el tokenizador común de BERT, solo cambiamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fImmBe-es7MS",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "5efefdeb-f12f-458e-fbd9-fc3233cba629"
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA1PyIHxtEl3",
    "colab_type": "text"
   },
   "source": [
    "Creemos una función que nos dice si tiene sentido o no la oración que continua."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ykgyF3hrtSes",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def evaluar_oraciones(primera,segunda):\n",
    "  encoding = tokenizer(primera, segunda, return_tensors='pt')\n",
    "  loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1])) # El label representa cual es la oración\n",
    "  #Nota logits[0,0] entrega el score que la oracion si sea la siguiente (que tan True)\n",
    "  #logits[0,1] entrega el score de que la oracion no sea la siguiente (que tan False)\n",
    "  # Se puede aplicar una SoftMax sobre estos resultados para que sean probabilidades\n",
    "  # Pero no es necesario.\n",
    "  if logits[0, 0] < logits[0, 1]:\n",
    "    print(\"La oración no tiene nada que ver\")\n",
    "  elif logits[0,0] > logits[0,1]:\n",
    "    print(\"La oración es una continuación\")\n",
    "  else:\n",
    "    print(\"No estoy seguro\")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-RxdXlvuPW9",
    "colab_type": "text"
   },
   "source": [
    "Probemos este código con algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BNNxshA5tK8B",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "9c8f9ac0-0ae8-46b3-f370-17cf3e2e4b78"
   },
   "source": [
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "evaluar_oraciones(prompt,next_sentence)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La oración no tiene nada que ver\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-QWK8kC0ujfM",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "55a82d60-c681-4514-8ee6-d26997a35465"
   },
   "source": [
    "prompt = \"I'm really hungry.\"\n",
    "next_sentence = \"I'm getting a BigMac.\"\n",
    "evaluar_oraciones(prompt,next_sentence)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La oración es una continuación\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IziEtXRvs0i",
    "colab_type": "text"
   },
   "source": [
    "## Esto funciona bastante bien, que pasa si quiero entrenarlo para una tarea en especifico?\n",
    "\n",
    "Para esto debemos fine-tunear el modelo con nuestros datos. Tomé [este](https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa) tutorial como referencia por si algún paso no queda lo suficientemente claro. Vamos a fine-tunear BERT para realizar sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecFw688fxbVE",
    "colab_type": "text"
   },
   "source": [
    "Lo primero es inicializar un modelo de BERT sin fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Go2YCpqZxmBi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from transformers import BertModel"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2mMoB7qxiTQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Creamos un modelo de BERT limpio\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#El mismo tokenizador de antes\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IONb3bWNxBi4",
    "colab_type": "text"
   },
   "source": [
    "Debemos entender como se le pasan los datos a BERT. Imaginemos que queremos agregar varias oraciones simultaneamente. Como lo hacemos con tensores si tienen largo distinto? La solucion a esto se llama padding, es decir agregar tokens para que todas las secuencias tengan el mismo largo. \\\n",
    "Pero esto podría traer problemas si es que BERT llegase a interpretar estos tokens como partes de la oración verdad? Para eso es que es necesario especificarle a BERT cuales son los tokens a los que les tiene que tomar atención.\n",
    "\n",
    "Por ejemplo, si tuviesemos un largo máximo de 12 tokens, para padear la oración 'I really enjoyed this movie a lot.' hariamos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5sVnR_TOw9e5",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "outputId": "d0ef6e7f-d800-474d-a931-78a9a0d8fea1"
   },
   "source": [
    "#Largo maximo de los tokens\n",
    "T = 12\n",
    "sentence = 'I really enjoyed this movie a lot.'\n",
    "#Step 1: Tokenizar\n",
    "tokens = tokenizer.tokenize(sentence) # ['i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.']\n",
    "#Step 2: Agregar [CLS] y [SEP]\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]'] # ['[CLS]','i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.', '[SEP]']\n",
    "#Step 3: Padear tokens\n",
    "padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))] #    ['[CLS]','i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.', '[SEP]', '[PAD]', ... , '[PAD]']\n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens] # [    1  , 1 ,    1    ,    1     ,   1   ,    1   ,  1 ,   1  ,  1 ,   1    ,    0   , ... ,    0   ] \n",
    "#Step 4: Segment ids: Estos representan cuando tienes 2 oraciones, la primera se llena con 0's y la segunda con 1's\n",
    "seg_ids = [0 for _ in range(len(padded_tokens))] # En este caso no la usaremos, ya que es solo 1 oración. Su representacion son solo 0's\n",
    "#Step 5: Cambiamos los tokens por su respectivo numero, CLS = 101, SEP = 102, etc...\n",
    "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "#Los cambiamos a tensores de pytorch antes de que entren al modelo, y es necesario agregarles una dimension extra.\n",
    "# Esta dimension representa cuantas oraciones estamos pasando\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "attn_mask = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "seg_ids   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "\n",
    "#Y al igual que antes podemos pasarselos a BERT\n",
    "hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask, token_type_ids = seg_ids)\n",
    "print(hidden_reps.shape)\n",
    "#Out: torch.Size([1, 12, 768])\n",
    "print(cls_head.shape)\n",
    "#Out: torch.Size([1, 768])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1853-AW09Up",
    "colab_type": "text"
   },
   "source": [
    "No está de más agregar que BERT tiene un maximo de 512 tokens por input, por lo que si queremos agregar un texto muy grande debemos o truncarlo o separarlo en 2.\\\n",
    "Ahora que aprendimos como paddear oraciones, utilizaremos el Stanford Sentiment Tree Bank dataset que contiene movie reviews con sentimiento positivo (1) y negativo (0).\\\n",
    "Primero crearemos una clase para cargar los datos, extendiendo la clase Dataset que viene con pytorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g1q9tjbz1tm4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    # Inicializacion de la clase\n",
    "    def __init__(self, filename, maxlen):\n",
    "        #Guardar los contenidos del dataframe\n",
    "        self.df = pd.read_csv(filename, delimiter = '\\t')\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Establecer el largo máximo\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    # Funcion auxiliar que retorna el largo del dataframe\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Seleccionamos la oracion y el label de este dataset en especifico.\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        #Realizamos todo el pre-procesamiento que explicamos anteriormente\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < self.maxlen: # Comparamos con la cantidad maxima de tokens que dimos\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] # Si es mas corta agregamos padding\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]']  # Si es mas larga la cortamos\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Utilizamos el tokenizador para pasarlos a id\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Pasamos a tensor de pytorch\n",
    "\n",
    "        #1 para los tokens no padeados, 0 si es padding\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVwxAu893Swd",
    "colab_type": "text"
   },
   "source": [
    "Creamos los dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L0WWC2Be3UtV",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creamos instancias del training y validation sets\n",
    "train_set = SSTDataset(filename = 'train.tsv', maxlen = 30)\n",
    "val_set = SSTDataset(filename = 'dev.tsv', maxlen = 30)\n",
    "\n",
    "#Creamos los dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
    "val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File train.tsv does not exist: 'train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-f0be202f8b20>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m#Creamos instancias del training y validation sets\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mtrain_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSSTDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'train.tsv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxlen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m30\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mval_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSSTDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'dev.tsv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxlen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m30\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-14-697ffca8f53b>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filename, maxlen)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxlen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;31m#Guardar los contenidos del dataframe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdelimiter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'\\t'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[1;31m#Initialize the BERT tokenizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bert-base-uncased'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Workspace\\Python\\Envs\\NLP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    674\u001B[0m         )\n\u001B[0;32m    675\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 676\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    677\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    678\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Workspace\\Python\\Envs\\NLP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    446\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 448\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    449\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    450\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Workspace\\Python\\Envs\\NLP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    878\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    879\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 880\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    881\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    882\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Workspace\\Python\\Envs\\NLP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1112\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"c\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1113\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"c\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1114\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1115\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"python\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Workspace\\Python\\Envs\\NLP\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1889\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"usecols\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1890\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1891\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1892\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1893\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] File train.tsv does not exist: 'train.tsv'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIHYBYjP38X3",
    "colab_type": "text"
   },
   "source": [
    "Ahora la parte más importante, generar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "beYZGXu_4BAy",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch.nn as nn\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert = True):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Creamos una instancia de BERT sin entrenamiento previo\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "        \n",
    "        #Con esto podemos bloquear el entrenamiento de BERT, para comparar incluyendo el entrenamiento de bert y sin\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #La capa para clasificar\n",
    "        #La idea es transformar una representacion de BERT (768 dimensiones) en 1 o 0 que representa el sentimiento\n",
    "        self.cls_layer = nn.Linear(768, 1).cuda()\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "        #Le pasamos el input al modelo BERT\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        #Obtenemos la representacion del token CLS\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Pasamos el token CLS por la capa de clasificacion\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkRDKgHj496y",
    "colab_type": "text"
   },
   "source": [
    "Utilizaremos binary cross-entropy loss y un descenso de gradiente estocastico"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G7lXDEbk5OAB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch.optim as optim\n",
    "#Creamos el classificador de sentimiento basado en BERT\n",
    "net_freezed = SentimentClassifier(freeze_bert = True)\n",
    "net_not_freezed = SentimentClassifier(freeze_bert = False)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti_freezed = optim.Adam(net_freezed.parameters(), lr = 2e-5)\n",
    "opti_not_freezed = optim.Adam(net_not_freezed.parameters(), lr = 2e-5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf2qL53wBN3_",
    "colab_type": "text"
   },
   "source": [
    "Agregamos funciones axuliares para medir el desempeño del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w6Vk-syTBSUb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "    \n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval() # Modo evaluacion del modelo, pesos no serán modificados\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    with torch.no_grad(): # Los gradientes no serán guardados tampoco\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT0-p2BE5c-T",
    "colab_type": "text"
   },
   "source": [
    "Y con la siguiente funcion entrenamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "32HzrhSG5euh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train(net, criterion, opti, train_loader, val_loader, epochs):\n",
    "    for ep in range(epochs): # Iterador de las epocas\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Enviamos los tensores a la GPU\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "\n",
    "            #Evaluamos nuestro modelo en la secuencia y la mask de atencion\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Calculamos la loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            # Ojo que si no tenemos freeze_bert en true, vamos a entrenar los parametros de bert tambien.\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % 100 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Train Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
    "        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
    "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep+1, val_acc, val_loss))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsXNXjQY6aKt",
    "colab_type": "text"
   },
   "source": [
    "Y finalmente entrenamos ambos modelos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EcMLMjiz6ccC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "outputId": "67852a69-c293-42ed-b785-d41d5760b193"
   },
   "source": [
    "epochs = 5\n",
    "train(net_freezed, criterion, opti_freezed, train_loader, val_loader, epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XxBUZUC6d1jC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "outputId": "4d07370e-c782-4b0a-8d5d-23ef46f0794b"
   },
   "source": [
    "train(net_not_freezed, criterion, opti_not_freezed, train_loader, val_loader, epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giFhU398k17w",
    "colab_type": "text"
   },
   "source": [
    "Comparando ambos entrenamientos, pasamos de 82% a 88% en 5 epocas. El primer entrenamiento solo entrenamos la capa de clasificacion mientras que en el segundo tambien modificamos los parametros de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exBniyWo8Ni9",
    "colab_type": "text"
   },
   "source": [
    "Este ultimo entrenamiento es lo que llamamos fine-tunning. Lo más importante de esto es que no necesitamos un super computador para poder mejorar las representaciones en una tarea especifica."
   ]
  }
 ]
}