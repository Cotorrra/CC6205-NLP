{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "minitarea4.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "pycharm-959495b3",
   "language": "python",
   "display_name": "PyCharm (CC6205-NLP)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4lL5hGw07yP",
    "colab_type": "text"
   },
   "source": [
    "# Minitarea 4\n",
    "\n",
    "Por favor, en las respuestas de desarrollo expliquen lo que están haciendo. En las preguntas que son con desarrollo matemático pongan todos los pasos del cálculo (déjenlo bonito porfis :D).\n",
    "\n",
    "Usen $\\LaTeX$ para las fórmulas matemáticas.\n",
    "\n",
    "En la parte de programación pueden usar lo que quieran, pero la auxiliar 3 les puede servir de *inspiración*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWXD3D7RYKJ-",
    "colab_type": "text"
   },
   "source": [
    "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n",
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ D, N, V, P \\}$ y se tiene un Hidden Markov Model con los siguientes parámetros estimados a partir de un corpus de entrenamiento:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "q(D|N,P) &= 0.4 \\\\\n",
    "q(D|w,P) &= 0 \\qquad \\forall w \\in S, w \\neq N \\\\\n",
    "e(the|D) &= 0.6\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Luego para la oración: `Ella walks to the red house`, se tiene una tabla de programación dinámica con los siguientes valores:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\pi(3,D,P)&=0.1\\\\\n",
    "\\pi(3,N,P)&=0.2\\\\\n",
    "\\pi(3,V,P)&=0.01\\\\\n",
    "\\pi(3,P,P)&=0.5\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Con esta información, calcule el valor de $\\pi(4,P,D)$. Puede dejar el resultado expresado como una fracción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EzgysW9kGi-",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "La siguiente es la definición recursiva del Algoritmo de Viterbi:\n",
    "$\\pi(k,u,v) = max_{w\\in S_{k−2}}(\\pi(k−1,w,u)\\times q(v|w,u) \\times e(x_k|v)) $\n",
    "\n",
    "Se evalúa $\\pi(4,P,D)$, notando que $x_4 = the$:\n",
    "$\\pi(4,P,D) = max_{w\\in S_{2}}(\\pi(3,w,P)\\times q(D | w, P) \\times e(the | D)) $\n",
    "\n",
    "La expresión $e(the | D)$ es independiente de $w$ por lo que se puede sacar de la expresión de $max$ y reemplazar su valor:\n",
    "$\\pi(4,P,D) = 0.6 \\times max_{w\\in S_{2}}(\\pi(3,w,P)\\times q(D | w, P))$\n",
    "\n",
    "Dado que $S_2 = \\{ D, N, V, P \\}$ se tiene que calcular los distintos valores de $w$ en \n",
    "$\\pi(3,w,P)\\times q(D | w, P)$ para determinar qué $w$ maximiza esa expresión, sin embargo notar que dado los datos estimados del corpus de entrenamiento se tiene que:\n",
    "\n",
    "\\begin{equation}\n",
    "  q(D | w, P) =\n",
    "  \\begin{cases}\n",
    "    0.4 & \\text{if $w = N$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Esto implica que el único $w$ que no hace que la expresión sea nula es $w = N$, por lo que ésta es la que maximiza la expresión. Por lo que la expresión resultante es:\n",
    "\n",
    "$\\pi(4,P,D) = 0.6 \\times (\\pi(3,N,P)\\times q(D | N, P))$\n",
    "\n",
    "$\\pi(4,P,D) = 0.6 \\times (0.2 \\times 0.4) = 0.048$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiwJb_vmkKLZ",
    "colab_type": "text"
   },
   "source": [
    "### Pregunta 2 (0.5 pts)\n",
    "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
    "* ¿Para qué tipo de tarea sirven? Dé dos ejemplo de este tipo de tarea y descríbalos brevemente.\n",
    "* ¿Qué modelos usan features? ¿Qué ventajas conlleva esto?\n",
    "* ¿Cómo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train?\n",
    "* ¿Qué le permite a los CRF realizar decisiones globales? ¿Qué diferencia con respecto a los MEMMs permite lograr esto? ¿Por qué los HMMs tampoco son capaces de tomar decisiones globales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9h5ow8OWF7y",
    "colab_type": "text"
   },
   "source": [
    "**Respuestas**\n",
    "* Estos modelos sirven para resolver problemas de Tagging, como el Part of Speech (PoS)\n",
    "o Name Entity Recognition (NER). Dada un documento o texto como input, la tarea de PoS es definir para cada palabra del input\n",
    "determinar si ésta es un Sustantivo, Adjetivo, Verbo, Adverbio u otro tipo, mientras que la tarea\n",
    "de NER es determinar qué partes del input pertenecen a algún tipo de entidad (sujeto).\n",
    "* Los modelos de MEMMs y CRF utilizan features, esto permite tener conocimiento de todas las etiquetas a clasificar, además permite obtener un\n",
    "manejo especial de las etiquetas a clasificar como actuar según prefijos o sufijos.\n",
    "* Los modelos HMMs separa las palabras con alto conteo ($\\geq5$) con las de bajo conteo y las clasifica utilizando reglas especiales\n",
    "determinadas según la tarea a resolver. Los modelos MEMMs y CRFs dentro del vector de features pueden contabilizar especialmente éstas etiquetas\n",
    "de baja frecuencia para agruparlas en tipos más grandes.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClRAHR95Y8aB",
    "colab_type": "text"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "### Pregunta 3 (1 pt)\n",
    "\n",
    "Considere la frase $w_{1..7}=$ \"El agua moja y el fuego quema\" $=[El, agua, moja, y, el, fuego, quema]$.\n",
    "\n",
    "La siguiente matriz de embeddings, donde la i-ésima fila corresponde al vector de embedding de la i-ésima palabra, ordenadas segun aparecen en la frase. (vectores de largo 2).\n",
    "\\begin{equation}\n",
    "E = \\begin{pmatrix}\n",
    "2 & 2\\\\\n",
    "0 & -2\\\\\n",
    "0 & 1\\\\\n",
    "-2 & 1\\\\\n",
    "-2 & 0\\\\\n",
    "2 & -1\\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Los siguientes 3 filtros\n",
    "\\begin{equation}\n",
    "U = \\begin{pmatrix}\n",
    "-1 & -1 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "0 & 0 & -1\\\\\n",
    "-1 & -1 & -1\\\\\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Y la función de activación\n",
    "\\begin{equation}\n",
    "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Usando estos paramátros calcule manualmente la representación (vector) resultante de aplicar la operación de convolución (sin padding) + max pooling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlQ30Arkq0u4",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Los 3 filtros son tamaño 6, los embeddings son de 2 dimensiones, de esta\n",
    "forma el tamaño de la ventana es 3. Esto dan los siguientes trigramas, con sus\n",
    "vectores de embedding unidos asociados:\n",
    "* \"El agua moja\" $\\to (2,2,0,-2,0,1)$\n",
    "* \"agua moja y\" $\\to (0,-2,0,1,-2,1)$\n",
    "* \"moja y el\" $\\to (0,1,-2,1,-2,0)$\n",
    "* \"y el fuego\" $\\to (-2,1,-2,0,2,-1)$\n",
    "* \"el fuego quema\" $\\to (-2,0,2,-1,0,2)$\n",
    "\n",
    "Cada uno de estos vectos se le calculará el producto punto de cada filtro de $U$.\n",
    "Lo cual nos da los siguientes valores:\n",
    "* $(2,2,0,-2,0,1) \\to ( 3,  2,  1)$\n",
    "* $(0,-2,0,1,-2,1)  \\to    ( 0, -1, -4)$\n",
    "* $(0,1,-2,1,-2,0)  \\to    ( 2,  2, -1)$\n",
    "* $(-2,1,-2,0,2,-1) \\to    (0,  1,  5)$\n",
    "* $(-2,0,2,-1,0,2)  \\to  ( 5,  3, -3)$\n",
    "\n",
    "Ahora cada valor del resultado se le aplicó la función de activación $tanh$\n",
    "(se aproximó a 3 cifras significativas):\n",
    "\n",
    "* $( 3,  2,  1) \\to (0.995,0.964,0.761)$\n",
    "* $( 0, -1, -4) \\to (0,-0.762,-0.999)$\n",
    "* $( 2,  2, -1) \\to (0.964,0.964,-0.762)$\n",
    "* $( 0,  1,  5) \\to (0,0.762,1)$\n",
    "* $( 5,  3, -3) \\to (1,0.995,-0.995)$\n",
    "\n",
    "Finalmente se hace el Pooling, que es escojer el máximo para cada columna\n",
    "de los resultados de cada filtro:\n",
    "\n",
    "* (1, 0.964, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj1V_sAzZCHY",
    "colab_type": "text"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Pregunta 4 (1 pt)\n",
    "Sea la siguiente oración: `El perro ladra` representada como una secuencia de vectores $x1,x2,x3$. Donde cada vector corresponde al word embedding de cada palabra, que a la vez es un vector de dos dimensiones\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x1 &= Embed(El)    &= [1, 0] \\\\\n",
    "x2 &= Embed(perro) &= [-1, 1] \\\\\n",
    "x3 &= Embed(ladra) &= [1,1]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Se tiene una red recurrente Elman: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
    "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Con\n",
    "\\begin{equation}\n",
    "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad \\vec{x}_i \\in \\mathbb{R}^{d_x}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s}\n",
    "\\end{equation}\n",
    "y donde los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
    "\n",
    "Sea\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_0 &= [0,0,0]\\\\\n",
    "W^x &= \\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "W^s &= \\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} \\\\\n",
    "\\vec{b} &= [0, 0, 0] \\\\\n",
    "g(x) &= ReLu(x) = max(0, x)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "Calcule manualmente los valores de $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M7sqIQV-Q3a",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Para cada palabra $\\vec{x}_i$ se calula la función $R_{SRNN}(\\vec{x}_{i},\\vec{s}_{i-1})$ para obtener el estado $\\vec{s}_i$,\n",
    "para este cálculo se utiliza el vector de embedding de ésta palabra.\n",
    "\n",
    "Luego, se realizan las multiplicaciones de matrices, la suma de éstos vectores resultantes para finalmente\n",
    "para obtener el estado siguiente utilizando la\n",
    "función de activación dada $g(x) = max(0,x)$.\n",
    "\n",
    "Con este estado se utiliza la función $O_{SRRN}$ para obtener el vector $\\vec{y}_i$\n",
    "\n",
    "Dado que se obtiene el estado $\\vec{s}_{i}$ se puede calcular el estado siguiente a éste utilizando\n",
    "$\\vec{x}_{i+1}.\n",
    "\n",
    "Se calcula en primera instancia $R_{SRNN}(\\vec{x}_1,\\vec{s_0})$ \n",
    "donde:\n",
    "$\\vec{x}_1 = Embed(El) = (1,0)$\n",
    "\\begin{equation}\n",
    "\\vec{s}_1 = R_{SRNN}(\\vec{x}_1,\\vec{s_0}) = ReLU(\\vec{s}_{0}W^s + \\vec{x}_1 W^x + \\vec{b}) \\\\\n",
    "= ReLU(\n",
    "\\begin{pmatrix}\n",
    "0,0,0\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} + \n",
    "\\begin{pmatrix}\n",
    "1,0\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} + (0,0,0))  \\\\ \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (0,0,0) + (0,0,1) + (0,0,0))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (0,0,1)) = (0,0,1)\n",
    "\\end{equation}\n",
    "\n",
    "Así: $\\vec{y}_1 = O_{SRNN}(\\vec{s}_1) = (0,0,1)$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "$\\vec{x}_2 = Embed(perro) = (-1,1)$\n",
    "\\begin{equation}\n",
    "\\vec{s}_2 = R_{SRNN}(\\vec{x}_2,\\vec{s}_1) = ReLU(\\vec{s}_{1}W^s + \\vec{x}_2 W^x + \\vec{b}) \\\\\n",
    "= ReLU(\n",
    "\\begin{pmatrix}\n",
    "0,0,1\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} + \n",
    "\\begin{pmatrix}\n",
    "-1,1\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} + (0,0,0))  \\\\ \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (1,1,1) + (1,-1,-1) + (0,0,0))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (2,0,0)) = (2,0,0)\n",
    "\\end{equation}\n",
    "\n",
    "Así: $\\vec{y}_2 = O_{SRNN}(\\vec{s}_2) = (2,0,0)$\n",
    "\n",
    "---\n",
    "\n",
    "$\\vec{x}_3 = Embed(ladra) = (1,1)$\n",
    "\\begin{equation}\n",
    "\\vec{s}_3 = R_{SRNN}(\\vec{x}_3,\\vec{s}_2) = ReLU(\\vec{s}_2W^s + \\vec{x}_3 W^x + \\vec{b}) \\\\\n",
    "= ReLU(\n",
    "\\begin{pmatrix}\n",
    "2,0,0\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} + \n",
    "\\begin{pmatrix}\n",
    "1,1\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} + (0,0,0))  \\\\ \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (2,0,2) + (1,-1,1) + (0,0,0))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= ReLU( (3,-1,3)) = (3,0,3)\n",
    "\\end{equation}\n",
    "\n",
    "Así: $\\vec{y}_2 = O_{SRNN}(\\vec{s}_2) = (3,0,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4rAT6ELxRZW",
    "colab_type": "text"
   },
   "source": [
    "### Pregunta 5 (0.5 pts)\n",
    "¿De qué forma las RNN y las CNN logran aprender representaciones específicas\n",
    "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden \n",
    "con los modelos que usan *features* diseñadas manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6AXbQSgA_t8",
    "colab_type": "text"
   },
   "source": [
    "**Respuesta**\n",
    "Aprenden a través de la refinación de sus parámetros a medidas que se va realizando\n",
    "el cómputo de la minimización de la loss durante las múltiples iteraciones del entrenamiento.\n",
    "Por lo que al utilizar algún modelo que utiliza features éste sólo necesita leer una única vez\n",
    "(contar palabras, largo, etc.) el set de entrenamiento para determinar los parámetros internos para\n",
    "resolver la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxQIuO8axTUa",
    "colab_type": "text"
   },
   "source": [
    "# Redes neuronales con PyTorch\n",
    "### Pregunta 6 (2 pts)\n",
    "En esta parte van a tener que implementar una red neuronal Feed Forward.\n",
    "Además, deberán entrenar el modelo usando uno de los datasets de TorchText.\n",
    "En la sección de la respuesta hay un esqueleto de lo que deben hacer,\n",
    "deberán completar los metodos del modelo y la parte asociada al entrenamiento la deben implementar ustedes.\n",
    "De todas formas, como les mencionamos en las auxiliares, el proceso de entrenamiento es bastante estándar,\n",
    "así que se pueden guiar en gran medida por los ejemplos que hemos visto y\n",
    "los que vamos a ver en las próximas auxiliares.\n",
    "\n",
    "### Bonus (0.5 pts)\n",
    "Agregue a la arquitectura una capa convolucional,\n",
    "para esto debe registrar el parametro $U$ en la red y realizar el computo\n",
    "de la convolución en el metodo forward de la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LVKEaQXZ3eGl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%capture\n",
    "# Nos aseguramos que torchtext este en la ultima version\n",
    "!pip install --upgrade torchtext"
   ],
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dJ-wrzFO5mCC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "outputId": "de898848-3def-4c0f-84c0-09896b34ab12"
   },
   "source": [
    "# Trabajen con el siguiente dataset\n",
    "import os\n",
    "from random import choice\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "os.makedirs(\".data\", exist_ok=True)\n",
    "train_dataset, test_dataset = AG_NEWS(root=\".data\")\n",
    "\n",
    "print(\"\\nUn ejemplo aleatorio\\n\", choice(train_dataset))"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:05, 23073.21lines/s]\n",
      "120000lines [00:08, 13527.96lines/s]\n",
      "7600lines [00:00, 14283.47lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Un ejemplo aleatorio\n",
      " (1, tensor([  499,  1319, 32717,     5,  3531,    12,  4522,     3,   742, 29573,\n",
      "           41,  4325,     4,  1447,  9824, 32717,     9,   707,     5,     3,\n",
      "         3531,    12, 12353,   336,  6662,  4522,     9,  7063,     6,   269,\n",
      "            5, 49901,  1122, 17623,    30,     3,  1493,     2]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wSHn-hYi8XN8",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "outputId": "46e96894-5243-4fd8-9898-2ff5b8d388e0"
   },
   "source": [
    "from pprint import pprint\n",
    "# Informacion relevante del dataset\n",
    "vocab = train_dataset.get_vocab()\n",
    "labels = train_dataset.get_labels()\n",
    "# El nombre de cada label lo pueden encontrar aqui\n",
    "# https://pytorch.org/text/datasets.html#ag-news, aunque no es necesario para \n",
    "# clasificar\n",
    "print(labels)\n",
    "# Un ejemplo convertido a texto\n",
    "pprint(\" \".join(vocab.itos[token] for token in choice(train_dataset)[1]))"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n",
      "('state street profit drops , job cuts coming boston ( reuters ) - state '\n",
      " \"street corp . , the world ' s biggest institutional asset manager , on \"\n",
      " 'tuesday said third-quarter profit fell 12 percent amid lower revenue and '\n",
      " 'higher expenses , sending its shares tumbling .')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXngUm9HxKvA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# De aca para abajo viene su respuesta, completen las funciones en la red\n",
    "# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n",
    "# cargar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, pad_idx, cnn_dim, hidden_size): # Reemplacen el *args por sus argumento\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            pad_idx)\n",
    "        self.cnn = nn.Conv1d(1, cnn_dim, 3*embed_dim,stride=embed_dim) # thx aux 4\n",
    "        self.fc1 = nn.Linear(cnn_dim, hidden_size) # thx aux 3\n",
    "        self.fc2 = nn.Linear(hidden_size, num_class)\n",
    "        # Aca deben registrar sus parametros. A lo menos necesitan\n",
    "        # una capa de embedding y un MLP basico (una capa lineal + softmax)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # (B, N, E) -> (B, E)\n",
    "        z = self.embedding(batch)\n",
    "        z = z.view(z.shape[0], 1, -1)\n",
    "        z = torch.tanh(self.cnn(z))\n",
    "        z = z.max(dim=-1).values\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        return torch.softmax(z,-1)"
   ],
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mPHyqgId96ra",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# El resto de su respuesta\n",
    "# Aca deben programar el entrenamiento de la red\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from itertools import zip_longest\n",
    "\n",
    "def generate_batch(batch):\n",
    "    return (\n",
    "        # En este caso como los labels son numeros, el tensor es de 1 dimension\n",
    "        # de tamanno batch_size\n",
    "        torch.tensor([item[0] for item in batch]),\n",
    "\n",
    "        # En este caso se retorna un tensor de 2 dimensiones, batch_size x N,\n",
    "        # donde N es mayor largo de los ejemplo en el batch. Aca se realiza\n",
    "        # padding de los ejemplos mas cortos.\n",
    "        torch.tensor(\n",
    "            list(\n",
    "                zip(\n",
    "                    *zip_longest(\n",
    "                        *[item[1] for item in batch], fillvalue=vocab[\"<pad>\"]\n",
    "                    )))),)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(train_dataset):\n",
    "\n",
    "    # Entranamos el modelo\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=generate_batch,\n",
    "    )\n",
    "    for i, (cls, text) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        cls, text = cls.to(device), text.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Ajustar el learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_dataset), train_acc / len(train_dataset)\n",
    "\n",
    "def test(test_dataset):\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch\n",
    "    )\n",
    "    for cls, text in data:\n",
    "        cls, text = cls.to(device), text.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            test_loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return test_loss / len(test_dataset), acc / len(test_dataset)"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 37 seconds\n",
      "\tLoss: 0.0433(train)\t|\tAcc: 25.0%(train)\n",
      "\tLoss: 0.0434(valid)\t|\tAcc: 25.0%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 39 seconds\n",
      "\tLoss: 0.0433(train)\t|\tAcc: 25.0%(train)\n",
      "\tLoss: 0.0434(valid)\t|\tAcc: 25.0%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 39 seconds\n",
      "\tLoss: 0.0433(train)\t|\tAcc: 25.0%(train)\n",
      "\tLoss: 0.0434(valid)\t|\tAcc: 25.0%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 38 seconds\n",
      "\tLoss: 0.0433(train)\t|\tAcc: 25.0%(train)\n",
      "\tLoss: 0.0434(valid)\t|\tAcc: 25.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARN_RATE = 4.0\n",
    "STEP_SIZE = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 100\n",
    "CNN_DIM = 35\n",
    "HIDDEN_SIZE = 200\n",
    "\n",
    "\n",
    "model = CNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_class=len(labels),\n",
    "    pad_idx=vocab[\"<pad>\"],\n",
    "    cnn_dim=CNN_DIM,\n",
    "    hidden_size=HIDDEN_SIZE).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, STEP_SIZE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_dataset)\n",
    "    valid_loss, valid_acc = test(test_dataset)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs // 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n",
    "    )\n",
    "    print(\n",
    "        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n",
    "        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n",
    "        f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}